# 为什么不同算法的训练曲线不一样？

## 📊 已修复的问题

### 1. ✅ WARMUP_STEPS 不一致（已修复）
- **之前**：
  - DQN: `WARMUP_STEPS = 4`
  - PPO: `WARMUP_STEPS = 32` ❌（8倍差异！）
- **现在**：
  - DQN: `WARMUP_STEPS = 4`
  - PPO: `WARMUP_STEPS = 4` ✅（已统一）

**影响**：PPO的预热步数过多会导致每个episode的有效学习步数减少，影响训练曲线。

---

## 🔍 导致曲线不同的正常原因

即使环境设置完全一致，不同算法的训练曲线仍然会不同，这是**正常且预期的**，原因如下：

### 1. **算法本质不同**

| 算法 | 类型 | 学习方式 | 特点 |
|------|------|---------|------|
| **DQN** | Off-policy | 从经验回放中学习 | 可以重用历史经验，样本效率高 |
| **PPO** | On-policy | 从当前策略收集的数据中学习 | 需要新数据，但更稳定 |
| **A2C** | On-policy | 单步更新，简单快速 | 样本效率较低，但实现简单 |
| **QTable** | Tabular | 查表法，无函数逼近 | 适合小状态空间，性能有限 |

**影响**：
- DQN可能更快收敛（因为重用历史数据）
- PPO可能更稳定（因为限制策略更新）
- A2C可能波动更大（因为单步更新）
- QTable性能最差（因为状态离散化粗糙）

### 2. **探索策略不同**

| 算法 | 探索方式 | 参数 |
|------|---------|------|
| **DQN** | Epsilon-greedy | `EPS_START=1.0`, `EPS_END=0.01`, `EPS_DECAY_STEPS=15000` |
| **PPO** | Entropy regularization | `ENTROPY_COEF_START=0.20`, `ENTROPY_COEF_END=0.05` |
| **A2C** | Entropy regularization | `ENTROPY_COEF=0.15` |
| **QTable** | Epsilon-greedy | `EPS_START=1.0`, `EPS_END=0.05`, `EPS_DECAY_STEPS=20000` |

**影响**：
- 不同的探索策略会导致不同的探索-利用平衡
- 探索越多，初期性能可能更差，但后期可能更好
- 探索越少，可能过早收敛到次优策略

### 3. **网络结构不同**

| 算法 | 网络结构 | 特点 |
|------|---------|------|
| **DQN** | 512→256→128 (LayerNorm + Dropout) | 更深更宽，学习能力强 |
| **PPO** | 256→128 (LayerNorm + Dropout) | 中等深度，稳定 |
| **A2C** | 128→128 (Tanh激活) | 较浅，简单 |
| **QTable** | 无网络（查表） | 无函数逼近能力 |

**影响**：
- 更深的网络可能学习更复杂的策略
- 不同的激活函数（ReLU vs Tanh）影响学习动态

### 4. **学习机制不同**

| 算法 | 更新方式 | 样本效率 |
|------|---------|---------|
| **DQN** | 批量更新（从replay buffer采样） | 高（重用历史数据） |
| **PPO** | 多轮更新（从当前rollout采样） | 中（只使用当前数据，但多轮更新） |
| **A2C** | 单轮更新（从当前episode采样） | 低（只用一次数据） |
| **QTable** | 单步更新（Q-learning） | 低（无函数逼近） |

**影响**：
- DQN可能更快学习（因为重用数据）
- PPO可能更稳定（因为多轮更新）
- A2C可能更慢（因为只用一次数据）

### 5. **随机性来源**

即使环境相同，以下随机性会导致不同结果：

1. **网络初始化**：每次训练时网络权重随机初始化
2. **动作选择**：epsilon-greedy和采样都有随机性
3. **经验回放采样**：DQN从buffer中随机采样
4. **PPO的mini-batch**：每次更新时随机打乱数据

**影响**：
- 相同的算法，不同的随机种子，结果也会不同
- 这是强化学习的正常现象

---

## 🎯 如何确保公平对比？

### ✅ 已统一的设置

1. **环境设置**：
   - ✅ 相同的测试案例（`bestest_air`）
   - ✅ 相同的动作空间（24个动作）
   - ✅ 相同的episode长度（96步 = 1天）
   - ✅ 相同的reward函数
   - ✅ 相同的初始化方式（`start_time=0`）
   - ✅ 相同的WARMUP_STEPS（4步）

2. **训练设置**：
   - ✅ 相同的总episode数（500）
   - ✅ 相同的评估频率（每20个episode）
   - ✅ 相同的gamma（0.99）

### ⚠️ 无法统一的差异（算法特性）

这些差异是**算法本身的特性**，无法也不应该统一：

1. **学习机制**：Off-policy vs On-policy
2. **探索策略**：Epsilon-greedy vs Entropy
3. **网络结构**：不同深度和宽度
4. **更新方式**：批量更新 vs 多轮更新 vs 单轮更新

---

## 📈 预期结果

即使环境完全相同，不同算法的曲线也应该不同：

1. **DQN**：
   - 可能更快收敛（重用历史数据）
   - 可能更稳定（经验回放）
   - 曲线可能更平滑

2. **PPO**：
   - 可能更稳定（限制策略更新）
   - 可能探索更充分（高熵系数）
   - 曲线可能波动更小

3. **A2C**：
   - 可能更慢（单轮更新）
   - 可能波动更大（样本效率低）
   - 曲线可能更不稳定

4. **QTable**：
   - 性能最差（状态离散化粗糙）
   - 可能无法学习复杂策略
   - 曲线可能最差

---

## 💡 建议

1. **多次运行取平均**：每个算法运行多次（如5次），取平均曲线对比
2. **设置随机种子**：可以添加随机种子设置，确保可复现性
3. **关注趋势而非绝对值**：关注曲线趋势（是否上升）而非具体数值
4. **对比最终性能**：关注训练结束时的最终性能，而非中间过程

---

## 🔧 可选：添加随机种子

如果需要可复现性，可以在每个文件开头添加：

```python
import random
import numpy as np
import torch

# 设置随机种子（可选，用于可复现性）
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
```

**注意**：设置随机种子后，每次运行结果会相同，但不同算法之间仍然会有差异（因为算法本质不同）。




