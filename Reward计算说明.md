# Reward 计算方式详解

## 📊 当前 Reward 计算逻辑

### 1. 为什么有 Target (22°C)？

**目标**：鼓励模型将温度控制在**22°C**（舒适区中心），而不是只要在20-24°C范围内就行。

**原因**：
- 22°C是人体最舒适的温度
- 如果只要求20-24°C，模型可能让温度在边界徘徊（20°C或24°C）
- 通过target机制，模型会学习将温度稳定在22°C附近

---

## 🎯 Reward 计算公式

### **舒适区内（20°C ≤ 温度 ≤ 24°C）**

```python
temp_dev_from_target = abs(room_temp_c - 22.0)  # 距离22°C的偏差
comfort = 5.0 - 0.75 * (temp_dev_from_target ** 2)
```

**关键点**：**舒适区间内的reward不是一样的！**

| 温度 | 距离22°C | Comfort Reward | 说明 |
|------|---------|----------------|------|
| **22°C** | 0°C | **5.0** | ✅ 最高奖励 |
| 21°C | 1°C | 4.25 | 接近目标 |
| 23°C | 1°C | 4.25 | 接近目标 |
| 20°C | 2°C | 2.0 | 边界，奖励较低 |
| 24°C | 2°C | 2.0 | 边界，奖励较低 |

**公式说明**：
- 22°C时：`comfort = 5.0 - 0.75 * 0² = 5.0`（最高）
- 21°C时：`comfort = 5.0 - 0.75 * 1² = 4.25`
- 20°C时：`comfort = 5.0 - 0.75 * 2² = 2.0`（边界最低）

---

### **舒适区外（温度 < 20°C 或 温度 > 24°C）**

```python
if room_temp_c < 20.0:
    temp_dev_from_boundary = 20.0 - room_temp_c  # 距离下边界的距离
else:  # room_temp_c > 24.0
    temp_dev_from_boundary = room_temp_c - 24.0  # 距离上边界的距离

comfort = -2.5 * (temp_dev_from_boundary ** 2)
```

**关键点**：**离舒适区越远，penalty越大（二次增长）**

| 温度 | 距离边界 | Comfort Reward | 说明 |
|------|---------|----------------|------|
| 20°C | 0°C | 0（边界） | 刚好在边界 |
| 19°C | 1°C | **-2.5** | 轻微惩罚 |
| 18°C | 2°C | **-10.0** | 中等惩罚 |
| 15°C | 5°C | **-62.5** | 严重惩罚 |
| 25°C | 1°C | **-2.5** | 轻微惩罚 |
| 26°C | 2°C | **-10.0** | 中等惩罚 |
| 30°C | 6°C | **-90.0** | 严重惩罚 |

**公式说明**：
- 19°C：`comfort = -2.5 * 1² = -2.5`
- 18°C：`comfort = -2.5 * 2² = -10.0`
- 15°C：`comfort = -2.5 * 5² = -62.5`（会被裁剪到-50.0）

---

### **极端温度额外惩罚**

```python
if room_temp_c < 18.0 or room_temp_c > 26.0:
    comfort -= 10.0  # 额外硬惩罚
```

**说明**：如果温度低于18°C或高于26°C，在原有惩罚基础上再减去10.0

---

## 📈 完整 Reward 组成

```python
reward = comfort + energy + smooth
```

### 1. **Comfort（舒适度）** - 主要部分
- 舒适区内：2.0 ~ 5.0（22°C时最高）
- 舒适区外：-∞ ~ 0（离边界越远越负）

### 2. **Energy（能耗惩罚）** - 次要部分
- 舒适区内：`-0.08 * (p_h + p_c) - 0.015 * p_f`
- 舒适区外：`-0.02 * (p_h + p_c) - 0.005 * p_f`（几乎不惩罚）

### 3. **Smooth（动作平滑）** - 很小
- `-0.005 * abs(obs[-1] - self.prev_action_norm)`

---

## 🎯 关键理解

### ✅ **舒适区间内reward不是一样的！**

- **22°C**：最高奖励 5.0
- **21°C或23°C**：4.25
- **20°C或24°C**：2.0（边界最低）

这鼓励模型将温度控制在22°C附近，而不是在边界徘徊。

### ✅ **离舒适区越远，penalty越大（二次增长）**

- 距离边界1°C：penalty = -2.5
- 距离边界2°C：penalty = -10.0
- 距离边界5°C：penalty = -62.5（会被裁剪到-50.0）

这确保模型优先保证温度在舒适区内。

---

## 📊 Reward 曲线示意图

```
Reward
  ↑
 5.0|     ● (22°C, 5.0)
    |    / \
 4.0|   /   \  (21°C, 4.25)  (23°C, 4.25)
    |  /     \
 2.0|●         ● (20°C, 2.0)  (24°C, 2.0)
    |/
  0 |───────────────────────────────
    |\         /
-2.5| \       /  (19°C, -2.5)  (25°C, -2.5)
    |  \     /
-10.0|   \   /  (18°C, -10.0)  (26°C, -10.0)
    |    \ /
-50.0|     ● (15°C, -50.0)  (30°C, -50.0)
    └────────────────────────────────→ 温度(°C)
    15  18  20  22  24  26  30
```

---

## 🔍 与旧版本对比

### 旧版本（统一奖励）
```python
if 20.0 <= room_temp_c <= 24.0:
    comfort = 3.0  # 所有舒适区内的温度都是3.0
```
**问题**：模型可能让温度在20°C或24°C徘徊，不够精确。

### 新版本（Target机制）
```python
if 20.0 <= room_temp_c <= 24.0:
    comfort = 5.0 - 0.75 * (abs(room_temp_c - 22.0) ** 2)
```
**优势**：鼓励模型将温度精确控制在22°C附近。

---

## 💡 总结

1. **Target (22°C)的作用**：让模型学习精确控制，而不是只满足边界条件
2. **舒适区间内reward不一样**：22°C最高(5.0)，边界最低(2.0)
3. **离舒适区越远penalty越大**：二次增长，确保优先保证舒适度
4. **能耗权重很低**：优先保证舒适度，能耗是次要考虑





